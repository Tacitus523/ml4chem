{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Welcome to this Pandas-tutorial\n",
    "\n",
    "After this tutorial you'll be more proficient using python and be able facilitate your data analysis.\n",
    "\n",
    "Pandas is a fast, powerful, flexible and easy to use  data analysis and manipulation tool, built on top of the Python programming language. It can be seen as the Python version of Excel. Its [documentation](https://pandas.pydata.org/docs/index.html) can be helpful in further problems.\n",
    "\n",
    "The tutorial is structured as follows:\n",
    "    \n",
    "    1. The pandas.Series object and the pandas.DataFrame in general\n",
    "    2. Overview over data in your DataFrame\n",
    "    3. Accessing data\n",
    "    4. Data Manipulation\n",
    "    5. Data Analysis\n",
    "    6. Data Filtering\n",
    "    7. Data Aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements\n",
    "\n",
    "We will use three external python libraries, pandas(obviously), numpy and seaborn, which have to be available in your python environment to use their functions. Try to import these three libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # Conventionally pandas gets imported with the alias pd\n",
    "import numpy as np # We'll use a tiny bit of numpy\n",
    "import seaborn as sns # For plotting\n",
    "\n",
    "pd.options.display.float_format = '{:,.2f}'.format # For formatting of floats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you encounter an error, you will have to install at least one of the libraries. Hopefully you followed the installation guide and have setup a virtual environment with conda.\n",
    "\n",
    "Close this notebook, activate your virtual environment in the terminal with `conda activate test_env`(or whichever name you chose for your environment) and install the packages with `conda install -c conda-forge seaborn`(seaborn depends on the other two libraries and will install them as well). Run the cell above again after the installation.\n",
    "\n",
    "If you don't have a conda environment, `pip install seaborn` should do the trick, but it's less flexible in the long run.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The pandas.Series object in general\n",
    "\n",
    "pandas.Series object are equivalent to a column in an excel spreadsheet and very similar to a list or a 1D numpy.array in python, but has some additional functionality. It can be easily constructed from both of them. At first we use a list and and give the series a title with the keyword \"name\" for the construction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measurement_times = [1, 5, 10, 120]\n",
    "\n",
    "series1 = pd.Series(measurement_times, name=\"time [s]\")\n",
    "print(series1)\n",
    "\n",
    "# The data can be given as a positional argument at position 0 (Python starts counting at 0)\n",
    "# or as a keyword-argument with the keyword 'data' at whatever position\n",
    "series2 = pd.Series(name=\"time [s]\", data=measurement_times)\n",
    "print(\"series1 is the same as series2:\", series1.equals(series2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The pandas.DataFrame object in general\n",
    "\n",
    "DataFrames are the pandas equivalent to excel spreadsheets. A DataFrame is an object with rows and column containing data. It's formed of several Series objects In contrast to a second common data type in python, the numpy.array, it's limited to two dimensions and therefore less suitable for high-dimensional calculations. Both have their strengths and weaknesses and are easily convertible into each other.\n",
    "\n",
    "Now we'll have a look at how to construct a simple example dataframe using 3 entries with 3 data points each: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entry0 = [1, 2, 3]\n",
    "entry1 = [4, 5, 6]\n",
    "entry2 = [7, 8, 9]\n",
    "\n",
    " # If given a nested list(lists within a list) or a 2D matrix as data, pandas will interpret the inner lists as row vectors\n",
    "df = pd.DataFrame([entry0, entry1, entry2], columns=['a', 'b', 'c'])\n",
    "print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other Construction methods\n",
    "\n",
    "A simple way to construct a dataframe with columnwise input, while defining column titels at the same time, is via a dictionary.\n",
    "\n",
    "To construct a Dataframe from existing data in a separate file, the pandas.Series.read_csv()-method is very useful. We'll use this dataframe in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = { \n",
    "    \"Article\": [\"Apple\", \"Pear\", \"Melon\", \"Lemon\"],\n",
    "    \"Price\": [1, 1.5, 2, 3],\n",
    "    \"Availability\": [True, False, False, True]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data_dict)\n",
    "print(df)\n",
    "\n",
    "molecule_df = pd.read_csv(\"eval_df.csv\", delimiter=\",\") # Make sure, that you saved the file \"eval_df.csv\" in the same folder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, that in the first example the integer values 1, 2 and 3 were converted to floating point numbers (floats), because 1.5 is a float and the whole column needs to be the same data type. Common python data types are `string, integer, float, bool, list, tuple, dict`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting an overview over an dataframe\n",
    "\n",
    "When starting to work with an unfamilar dataset it's very useful to get an overview of what you are dealing with. The beginning of a dataframe can be accessed with the `.head()`-method to get a grasp of the content of the dataframe. \n",
    "The name and the data type of columns can be checked with the `.columns` and the  `.dtypes` property of a dataframe.\n",
    "The amount of entries in a dataframe is available with the pythons `len()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(molecule_df.head(4))\n",
    "print(\"Column names:\", molecule_df.columns)\n",
    "print(molecule_df.dtypes)\n",
    "print(\"Amount of entries:\", len(molecule_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another common interest is, which unique values occur in a dataframe, how many unqique values there are and how often each unqique value occurs. These properties of a dataframe can be investigated with the `unique()`, `nunique()` and the `value_counts`-method for serieses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unique atom indices:\", molecule_df[\"at_idxs\"].unique()) \n",
    "print(\"Amount of unique molecule indices:\", molecule_df[\"mol_idxs\"].nunique())\n",
    "print(\"Values counts of atom indices:\")\n",
    "print(molecule_df[\"at_idxs\"].value_counts())\n",
    "\n",
    "print(\"Amount of unqiue energies:\", molecule_df[\"energies\"].nunique())\n",
    "print(\"Amount of unqiue charges:\", molecule_df[\"charges\"].nunique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good overviews of a dataframe over multiple properties and statistics at once are given by the `.info()` and the `.describe()` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "molecule_df.info()\n",
    "print(\"\\n\\n\")\n",
    "print(molecule_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualisation of the dataset is also very useful for getting an overview. Seaborns pair plots can help you to spotting patterns and correlation between columns.\n",
    "\n",
    "We'll import a new example data frame about penguin species and visualise the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "penguin_df = sns.load_dataset(\"penguins\")\n",
    "sns.pairplot(penguin_df, hue=\"species\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accessing single or multiple elements\n",
    "\n",
    "Generally there are two way to access data in a dataframe: via the names of the columns and via the index position with the `.iloc[]` method (integer location).\n",
    "\n",
    "*Remember: Access to list elements via indices or slices* \n",
    "```python \n",
    "list1 = [1,2,3,4]\n",
    "list1[0] # accesses 1 (python is zero-indexed)\n",
    "list1[1:3] # accesses [2,3] (the end of the slice is excluded from the result)\n",
    "list1[0:3] # accesses [1,2,3] (identical to [:3])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--original dataframe--\")\n",
    "print(df)\n",
    "\n",
    "print(\"\\n--whole column Price--\")\n",
    "print(df[\"Price\"]) # Access the columns via the column name\n",
    "print(df.Price) # For names without a space you can also acces the columns this way\n",
    "print(\"Note that the type of the df is\", type(df), \"but the type of the column is\", type(df[\"Price\"]))\n",
    "print(\"\\n--index 2 at column Price--\")\n",
    "print(df[\"Price\"][2]) # Access an element via column- und indexname\n",
    "print(\"Note that the type of the element is\", type(df[\"Price\"][2]))\n",
    "\n",
    "print(\"\\n--Article/Price subframe--\")\n",
    "target_columns = [\"Article\", \"Price\"]\n",
    "print(df[target_columns]) # Access the columns a list of column_names\n",
    "print(\"Note that the type of the element is\", type(df[target_columns]))\n",
    "\n",
    "print(\"\\n--row at index 0--\")\n",
    "print(df.iloc[0])\n",
    "print(\"Note that the type of the row is\", type(df.iloc[0]))\n",
    "print(\"\\n--element at row index 0 and column index 1--\")\n",
    "print(df.iloc[0,2]) # Access elements with df.iloc['row', 'column']\n",
    "print(\"Note that the type of the element is\", type(df.iloc[0,2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extending Dataframes\n",
    "\n",
    "We already know how to construct a new series. Adding it to our existing dataframe works the same way as adding a new key-value pair to dictionary with a new name and the associated values for each entry. The length of the list of new values has therefore to be equal to the length of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"In Stock\"] = [10, 0, 0, 13]\n",
    "print(df)\n",
    "# df[\"In Stock\"] = [10, 11, 12, 13, 14, 15, 16] would throw an error as it contains more values than the length of the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dealing with missing values\n",
    "\n",
    "With real-word data you will often encounter missing values in a dataset, where a measurement was not available or an operation could not be executed, like converting strings to numerical values, calculations resulting in near infinite numbers or dividing by 0. In this event you will find `NaN`s(Not a Number) in your dataframe. These can be handeled in different ways. You can either drop any column or row containing NaNs with the `.dropna()` method or fill Nans with `.fillna()`. dropna() requieres and the additional argument `axis`, which decides if the column(axis=0) or the row(axis=1) with NaNs will be dropped. When .fillna() you will need to specify the value which will replace NaNs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"df with NaN\")\n",
    "df[\"Calories\"] = [89, np.nan, 120, 101]\n",
    "print(df, end=\"\\n\\n\")\n",
    "\n",
    "print(\"df with dropped rows\")\n",
    "df_without_nan_rows = df.dropna(axis=0)\n",
    "print(df_without_nan_rows, end=\"\\n\\n\")\n",
    "\n",
    "print(\"df with dropped columns\")\n",
    "df_without_nan_columns = df.dropna(axis=1)\n",
    "print(df_without_nan_columns, end=\"\\n\\n\")\n",
    "\n",
    "print(\"df with filled NaN\")\n",
    "df_with_filled_nan = df.fillna(0)\n",
    "print(df_with_filled_nan)\n",
    "\n",
    "# Note that most dataframe operations won't affect the original dataframe, that they are applied on.\n",
    "# So keep the effect, they have to be saved to a new variable or overwrite an old one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Manipulation\n",
    "\n",
    "You can directly do calculations on all elements of a dataframe. Assume there was a 5% tax on fruits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Price_with_Tax\"] = df[\"Price\"]*1.05\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can apply the `.map()`method to change occurences of certain values to another one, which might be more descriptive in your opinion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[\"Availability\"])\n",
    "mapping_dict = {False: \"No\", True: \"Yes\"}\n",
    "print(df[\"Availability\"].map(mapping_dict)) # Maps occurences of False to \"No\" and occurences of True to \"Yes\"\n",
    "\n",
    "print(df[\"Article\"])\n",
    "print(df[\"Article\"].map({\"Apple\": \"Apfel\", \"Melon\": \"Melone\"})) # Missing mappings become NaNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data manipulation based on a condition can be achieved with the `.apply()` function, which applies a custom function to a column(with axis=0) or a row(with axis=1). This function is very flexible and can do much more though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Adjust Availability based on the In Stock value\")\n",
    "print(df[\"Availability\"])\n",
    "def transform_row(row):\n",
    "    if row[\"In Stock\"] > 0:\n",
    "        row[\"Availability\"] = \"For sure\"\n",
    "    else:\n",
    "        row[\"Availability\"] = \"Currently not\"\n",
    "    return row[\"Availability\"]\n",
    "    \n",
    "print(df.apply(transform_row, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Analysis\n",
    "\n",
    "Sums, Mean, Standard Deviation and Variance are also easily availabe for dataframes and series. For Dataframes the axis can be specified to calculate along a row or the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df)\n",
    "print(\"Sum: \", df[\"Price\"].sum())\n",
    "print(\"Mean: \", df[\"Price\"].mean())\n",
    "print(\"Std: \", df[\"Price\"].std())\n",
    "print(\"Var: \", df[\"Price\"].var())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Filtering\n",
    "\n",
    "Dataframes support `boolean indexing`, which means that you choose which rows to select with a list of boolean values.\n",
    "Also lists with boolean values can be obtained by checking, if a series fulfills a condition.\n",
    "By combining these two properties we can effectively filter our data by our chosen conditions.\n",
    "\n",
    "Conditions can be combined with the and-operator `&`, the or-operator `|` and the not-operator `~` or simply the commands `and`, `or` and `not` themselves to form more complex conditions.\n",
    "\n",
    "Here we get the boolean array, where the price is grater than two, and then get the entries, which fulfill the condition. We only show relevant columns for each result.\n",
    "\n",
    "We also try another condition, where we only want to see articles, whose name contain an 'on'.\n",
    "\n",
    "The last example executes a complex filter in one go.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original\n",
    "print(df[[\"Article\", \"Price\", \"Calories\", \"Availability\"]], end=\"\\n\\n\")\n",
    "\n",
    "# Filtered with simple boolean array\n",
    "print(\"Greater two?\")\n",
    "boolean_array = (df.Price > 2)\n",
    "print(boolean_array)\n",
    "filtered_df = df[boolean_array]\n",
    "print(filtered_df[[\"Article\", \"Price\"]], end=\"\\n\\n\")\n",
    "\n",
    "# String condition, article name must contain 'on'\n",
    "print(\"Name with 'on'?\")\n",
    "print(df[df.Article.str.contains(\"on\")].Article, end=\"\\n\\n\")\n",
    "\n",
    "# Filtered with complex condition\n",
    "# Only access cheap articles or with a lot of calories, which are not unavailable\n",
    "print(\"Complex requirement fullfilled?\")\n",
    "print(df[((df.Price <= 2.0) | (df.Calories >= 100)) & ~(df.Availability == False)][[\"Article\", \"Price\", \"Calories\", \"Availability\"]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Aggregation\n",
    "\n",
    "The last functionality we'll have a look at today is data aggregation. This is done via the `groupby()` function, which takes a column name or a list of column names as argument and returns an object, where all entries with the same value in the given column are summarized into one entry. This object by itself can't be easily printed, as it's a higher dimensional object, but we can combine it with an aggregation function, like `.first(), .last(), .mean(), .median(), .min(), .max(), sum(), .std(), .var()` or a custom function(for more see [here](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html)), to e.g. get the first value/mean/minimal/maximal value of each of the possible values of the column that you grouped by. The higher dimensional object collapses to a simple 2D representation again.\n",
    "\n",
    "This is the most advanced function, that we'll have a look at today, so let's have a look what happens here.\n",
    "\n",
    "We'll return to the `molecule_df`, which contained molecular and atomic properties, and aggregate it by the molecular index at first the access the molecular property `energies`.\n",
    "\n",
    "Also we can check how much the `charges` of each atom index vary in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "molecule_aggregate = molecule_df.groupby(\"mol_idxs\")\n",
    "molecule_energies = molecule_aggregate[\"energies\"].first()\n",
    "print(\"Type of the aggregate:\", type(molecule_aggregate))\n",
    "print(\"Type of the aggregate after aggregation_function application\", type(molecule_energies))\n",
    "print(\"Length of the molecular energies:\", len(molecule_energies))\n",
    "print(\"Unique molecule idxs:\", molecule_df[\"mol_idxs\"].nunique())\n",
    "print(molecule_energies.head())\n",
    "\n",
    "atom_aggregate = molecule_df.groupby(\"at_idxs\")\n",
    "charge_variance = atom_aggregate[\"charges\"].var()\n",
    "\n",
    "\n",
    "print(charge_variance*100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can save Dataframes as .csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"test_tabelle_ml4chem.csv\" \n",
    "df.to_csv(path, sep=\";\", index=False) # sep delimites with semicolon, index=false drops the index-column\n",
    "df_reloaded = pd.read_csv(path, sep=\";\")\n",
    "print(df_reloaded)\n",
    "\n",
    "# Delete the file again as this wass just a demonstration\n",
    "import os\n",
    "os.remove(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**That's it**\n",
    "\n",
    "Thanks for participating in the Pandas crash course. There are countless further function, especially in combination with numpy, to execute complicated calculations and transformations in a few lines of code. Once you have your data in a readable format, is can be quickly analyzed descriptively. Together with matplotlib a well formatted and visually pleasing depiction is also easily createable automatically.\n",
    "\n",
    "Once the script is done, adjustments for small format changes for multiple data sets are quickly and consistently done without frustration to have to go to all formatting details again. Transfer to other data sets is as easy as changing the path variable.\n",
    "\n",
    "Don't forget that Pandas is a popular library and well documented, so if you run in any problems, they are likely to be well described somewhere."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your turn**\n",
    "\n",
    "If you want to try to apply your acquired knowledge: Try your hand at the pandas exercises in the second notebook."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "09479eb26e9c2dfedc5b750a5f4404419e7e88fc081728cd256fd3e13a96b5b1"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
