{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Welcome to this Pandas-tutorial\n",
    "\n",
    "After this tutorial you'll be more proficient using python and be able facilitate your data analysis.\n",
    "\n",
    "Pandas is a fast, powerful, flexible and easy to use  data analysis and manipulation tool, built on top of the Python programming language. It can be seen as the Python version of Excel. Its [documentation](https://pandas.pydata.org/docs/index.html) can be helpful in further problems.\n",
    "\n",
    "The tutorial is structured as follows:\n",
    "    \n",
    "    1. The pandas.Series object and the pandas.DataFrame in general\n",
    "    2. Overview over data in your DataFrame\n",
    "    3. Accessing data\n",
    "    4. Data Manipulation\n",
    "    5. Data Analysis\n",
    "    6. Data Filtering\n",
    "    7. Data Aggregation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The pandas.Series object in general\n",
    "\n",
    "pandas.Series object are equivalent to a column in an excel spreadsheet and very similar to a list or a 1D numpy.array in python, but has some additional functionality. It can be easily constructed from both of them. At first we use a list and and give the series a title with the keyword \"name\" for the construction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      1\n",
      "1      5\n",
      "2     10\n",
      "3    120\n",
      "Name: time [s], dtype: int64\n",
      "series1 is the same as series2: True\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd # Conventionally pandas gets imported with the alias pd\n",
    "import numpy as np # We'll use a little bit of numpy\n",
    "\n",
    "measurement_times = [1, 5, 10, 120]\n",
    "\n",
    "series1 = pd.Series(measurement_times, name=\"time [s]\")\n",
    "print(series1)\n",
    "\n",
    "# The data can be given as a positional argument at position 0 (Python starts counting at 0)\n",
    "# or as a keyword-argument with the keyword 'data' at whatever position\n",
    "series2 = pd.Series(name=\"time [s]\", data=measurement_times)\n",
    "print(\"series1 is the same as series2:\", series1.equals(series2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The pandas.DataFrame object in general\n",
    "\n",
    "DataFrames are the pandas equivalent to excel spreadsheets. A DataFrame is an object with rows and column containing data. It's formed of several Series objects In contrast to a second common data type in python, the numpy.array, it's limited to two dimensions and therefore less suitable for high-dimensional calculations. Both have their strengths and weaknesses and are easily convertible into each other.\n",
    "\n",
    "Now we'll have a look at how to construct a simple example dataframe using 3 entries with 3 data points each: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   a  b  c\n",
      "0  1  2  3\n",
      "1  4  5  6\n",
      "2  7  8  9\n"
     ]
    }
   ],
   "source": [
    "entry0 = [1, 2, 3]\n",
    "entry1 = [4, 5, 6]\n",
    "entry2 = [7, 8, 9]\n",
    "\n",
    " # If given a nested list(lists within a list) or a 2D matrix as data, pandas will interpret the inner lists as row vectors\n",
    "df = pd.DataFrame([entry0, entry1, entry2], columns=['a', 'b', 'c'])\n",
    "print(df)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other Construction methods\n",
    "\n",
    "A simple way to construct a dataframe with columnwise input, while defining column titels at the same time, is via a dictionary.\n",
    "\n",
    "To construct a Dataframe from existing data in a separate file, the pandas.Series.read_csv()-method is very useful. We'll use this dataframe in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Article  Price  Availability\n",
      "0   Apple   1.00          True\n",
      "1    Pear   1.50         False\n",
      "2   Melon   2.00         False\n",
      "3   Lemon   3.00          True\n"
     ]
    }
   ],
   "source": [
    "data_dict = { \n",
    "    \"Article\": [\"Apple\", \"Pear\", \"Melon\", \"Lemon\"],\n",
    "    \"Price\": [1, 1.5, 2, 3],\n",
    "    \"Availability\": [True, False, False, True]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data_dict)\n",
    "print(df)\n",
    "\n",
    "molecule_df = pd.read_csv(\"eval_df.csv\", delimiter=\",\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, that in the first example the integer values 1, 2 and 3 were converted to floating point numbers (floats), because 1.5 is a float and the whole column needs to be the same data type. Common python data types are `string, integer, float, bool, list, tuple, dict`.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting an overview over an dataframe\n",
    "\n",
    "When starting to work with an unfamilar dataset it's very useful to get an overview of what you are dealing with. The beginning of a dataframe can be accessed with the `.head()`-method to get a grasp of the content of the dataframe. \n",
    "The name and the data type of columns can be checked with the `.columns` and the  `.dtypes` property of a dataframe.\n",
    "The amount of entries in a dataframe is available with the pythons `len()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   mol_idxs  at_idxs  energies  charges  charge_predictions\n",
      "0         0        0      0.14    -0.10               -0.08\n",
      "1         0        1      0.14    -0.08               -0.17\n",
      "2         0        2      0.14     0.00                0.01\n",
      "3         0        3      0.14    -0.01               -0.01\n",
      "4         0        4      0.14    -0.00                0.00\n",
      "Column names: Index(['mol_idxs', 'at_idxs', 'energies', 'charges', 'charge_predictions'], dtype='object')\n",
      "mol_idxs                int64\n",
      "at_idxs                 int64\n",
      "energies              float64\n",
      "charges               float64\n",
      "charge_predictions    float64\n",
      "dtype: object\n",
      "Amount of entries: 203220\n"
     ]
    }
   ],
   "source": [
    "print(molecule_df.head())\n",
    "print(\"Column names:\", molecule_df.columns)\n",
    "print(molecule_df.dtypes)\n",
    "print(\"Amount of entries:\", len(molecule_df))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another common interest is, which unique values occur in a dataframe, how many unqique values there are and how often each unqique value occurs. These properties of a dataframe can be investigated with the `unique()`, `nunique()` and the `value_counts`-method for serieses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique atom indices: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      "Amount of unique molecule indices: 13548\n",
      "Values counts of atom indices:\n",
      "0     13548\n",
      "1     13548\n",
      "2     13548\n",
      "3     13548\n",
      "4     13548\n",
      "5     13548\n",
      "6     13548\n",
      "7     13548\n",
      "8     13548\n",
      "9     13548\n",
      "10    13548\n",
      "11    13548\n",
      "12    13548\n",
      "13    13548\n",
      "14    13548\n",
      "Name: at_idxs, dtype: int64\n",
      "Amount of unqiue energies: 13548\n",
      "Amount of unqiue charges: 115841\n"
     ]
    }
   ],
   "source": [
    "print(\"Unique atom indices:\", molecule_df[\"at_idxs\"].unique()) \n",
    "print(\"Amount of unique molecule indices:\", molecule_df[\"mol_idxs\"].nunique())\n",
    "print(\"Values counts of atom indices:\")\n",
    "print(molecule_df[\"at_idxs\"].value_counts())\n",
    "\n",
    "print(\"Amount of unqiue energies:\", molecule_df[\"energies\"].nunique())\n",
    "print(\"Amount of unqiue charges:\", molecule_df[\"charges\"].nunique())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good overviews of a dataframe over multiple properties and statistics at once are given by the `.info()` and the `.describe()` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 203220 entries, 0 to 203219\n",
      "Data columns (total 5 columns):\n",
      " #   Column              Non-Null Count   Dtype  \n",
      "---  ------              --------------   -----  \n",
      " 0   mol_idxs            203220 non-null  int64  \n",
      " 1   at_idxs             203220 non-null  int64  \n",
      " 2   energies            203220 non-null  float64\n",
      " 3   charges             203220 non-null  float64\n",
      " 4   charge_predictions  203220 non-null  float64\n",
      "dtypes: float64(3), int64(2)\n",
      "memory usage: 7.8 MB\n",
      "\n",
      "\n",
      "\n",
      "        mol_idxs    at_idxs   energies    charges  charge_predictions\n",
      "count 203,220.00 203,220.00 203,220.00 203,220.00          203,220.00\n",
      "mean    6,773.50       7.00       0.02      -0.07               -0.07\n",
      "std     3,910.98       4.32       0.02       0.16                0.15\n",
      "min         0.00       0.00      -0.02      -0.83               -0.80\n",
      "25%     3,386.75       3.00       0.00      -0.09               -0.08\n",
      "50%     6,773.50       7.00       0.02      -0.01               -0.01\n",
      "75%    10,160.25      11.00       0.03       0.01                0.01\n",
      "max    13,547.00      14.00       0.14       0.06                0.09\n"
     ]
    }
   ],
   "source": [
    "molecule_df.info()\n",
    "print(\"\\n\\n\")\n",
    "print(molecule_df.describe())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accessing single or multiple elements\n",
    "\n",
    "Generally there are two way to access data in a dataframe: via the names of the columns and via the index position with the `.iloc[]` method (integer location).\n",
    "\n",
    "*Remember: Access to list elements via indices or slices* \n",
    "```python \n",
    "list1 = [1,2,3,4]\n",
    "list1[0] # accesses 1 (python is zero-indexed)\n",
    "list1[1:3] # accesses [2,3] (the end of the slice is excluded from the result)\n",
    "list1[0:3] # accesses [1,2,3] (identical to [:3])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--original dataframe--\n",
      "  Article  Price  Availability\n",
      "0   Apple   1.00          True\n",
      "1    Pear   1.50         False\n",
      "2   Melon   2.00         False\n",
      "3   Lemon   3.00          True\n",
      "\n",
      "--whole column Price--\n",
      "0   1.00\n",
      "1   1.50\n",
      "2   2.00\n",
      "3   3.00\n",
      "Name: Price, dtype: float64\n",
      "0   1.00\n",
      "1   1.50\n",
      "2   2.00\n",
      "3   3.00\n",
      "Name: Price, dtype: float64\n",
      "Note that the type of the df is <class 'pandas.core.frame.DataFrame'> but the type of the column is <class 'pandas.core.series.Series'>\n",
      "\n",
      "--index 2 at column Price--\n",
      "2.0\n",
      "Note that the type of the element is <class 'numpy.float64'>\n",
      "\n",
      "--Article/Price subframe--\n",
      "  Article  Price\n",
      "0   Apple   1.00\n",
      "1    Pear   1.50\n",
      "2   Melon   2.00\n",
      "3   Lemon   3.00\n",
      "Note that the type of the element is <class 'pandas.core.frame.DataFrame'>\n",
      "\n",
      "--row at index 0--\n",
      "Article         Apple\n",
      "Price            1.00\n",
      "Availability     True\n",
      "Name: 0, dtype: object\n",
      "Note that the type of the row is <class 'pandas.core.series.Series'>\n",
      "\n",
      "--element at row index 0 and column index 1--\n",
      "True\n",
      "Note that the type of the element is <class 'numpy.bool_'>\n"
     ]
    }
   ],
   "source": [
    "print(\"--original dataframe--\")\n",
    "print(df)\n",
    "\n",
    "print(\"\\n--whole column Price--\")\n",
    "print(df[\"Price\"]) # Access the columns via the column name\n",
    "print(df.Price) # For names without a space you can also acces the columns this way\n",
    "print(\"Note that the type of the df is\", type(df), \"but the type of the column is\", type(df[\"Price\"]))\n",
    "print(\"\\n--index 2 at column Price--\")\n",
    "print(df[\"Price\"][2]) # Access an element via column- und indexname\n",
    "print(\"Note that the type of the element is\", type(df[\"Price\"][2]))\n",
    "\n",
    "print(\"\\n--Article/Price subframe--\")\n",
    "target_columns = [\"Article\", \"Price\"]\n",
    "print(df[target_columns]) # Access the columns a list of column_names\n",
    "print(\"Note that the type of the element is\", type(df[target_columns]))\n",
    "\n",
    "print(\"\\n--row at index 0--\")\n",
    "print(df.iloc[0])\n",
    "print(\"Note that the type of the row is\", type(df.iloc[0]))\n",
    "print(\"\\n--element at row index 0 and column index 1--\")\n",
    "print(df.iloc[0,2]) # Access elements with df.iloc['row', 'column']\n",
    "print(\"Note that the type of the element is\", type(df.iloc[0,2]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extending Dataframes\n",
    "\n",
    "We already know how to construct a new series. Adding it to our existing dataframe works the same way as adding a new key-value pair to dictionary with a new name and the associated values for each entry. The length of the list of new values has therefore to be equal to the length of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Article  Price  Availability  In Stock\n",
      "0   Apple   1.00          True        10\n",
      "1    Pear   1.50         False         0\n",
      "2   Melon   2.00         False         0\n",
      "3   Lemon   3.00          True        13\n"
     ]
    }
   ],
   "source": [
    "df[\"In Stock\"] = [10, 0, 0, 13]\n",
    "print(df)\n",
    "# df[\"In Stock\"] = [10, 11, 12, 13, 14, 15, 16] would throw an error as it contains more values than the length of the dataframe"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dealing with missing values\n",
    "\n",
    "With real-word data you will often encounter missing values in a dataset, where a measurement was not available or an operation could not be executed, like converting strings to numerical values, calculations resulting in near infinite numbers or dividing by 0. In this event you will find `NaN`s(Not a Number) in your dataframe. These can be handeled in different ways. You can either drop any column or row containing NaNs with the `.dropna()` method or fill Nans with `.fillna()`. dropna() requieres and the additional argument `axis`, which decides if the column(axis=0) or the row(axis=1) with NaNs will be dropped. When .fillna() you will need to specify the value which will replace NaNs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df with NaN\n",
      "  Article  Price  Availability  In Stock  Calories\n",
      "0   Apple   1.00          True        10     89.00\n",
      "1    Pear   1.50         False         0       NaN\n",
      "2   Melon   2.00         False         0    120.00\n",
      "3   Lemon   3.00          True        13    101.00\n",
      "\n",
      "df with dropped rows\n",
      "  Article  Price  Availability  In Stock  Calories\n",
      "0   Apple   1.00          True        10     89.00\n",
      "2   Melon   2.00         False         0    120.00\n",
      "3   Lemon   3.00          True        13    101.00\n",
      "\n",
      "df with dropped columns\n",
      "  Article  Price  Availability  In Stock\n",
      "0   Apple   1.00          True        10\n",
      "1    Pear   1.50         False         0\n",
      "2   Melon   2.00         False         0\n",
      "3   Lemon   3.00          True        13\n",
      "\n",
      "df with filled NaN\n",
      "  Article  Price  Availability  In Stock  Calories\n",
      "0   Apple   1.00          True        10     89.00\n",
      "1    Pear   1.50         False         0      0.00\n",
      "2   Melon   2.00         False         0    120.00\n",
      "3   Lemon   3.00          True        13    101.00\n"
     ]
    }
   ],
   "source": [
    "print(\"df with NaN\")\n",
    "df[\"Calories\"] = [89, np.nan, 120, 101]\n",
    "print(df, end=\"\\n\\n\")\n",
    "\n",
    "print(\"df with dropped rows\")\n",
    "df_without_nan_rows = df.dropna(axis=0)\n",
    "print(df_without_nan_rows, end=\"\\n\\n\")\n",
    "\n",
    "print(\"df with dropped columns\")\n",
    "df_without_nan_columns = df.dropna(axis=1)\n",
    "print(df_without_nan_columns, end=\"\\n\\n\")\n",
    "\n",
    "print(\"df with filled NaN\")\n",
    "df_with_filled_nan = df.fillna(0)\n",
    "print(df_with_filled_nan)\n",
    "\n",
    "# Note that most dataframe operations won't affect the original dataframe, that they are applied on.\n",
    "# So keep the effect, they have to be saved to a new variable or overwrite an old one"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Manipulation\n",
    "\n",
    "You can directly do calculations on all elements of a dataframe. Assume there was a 5% tax on fruits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Article  Price  Availability  In Stock  Calories  Price_with_Tax\n",
      "0   Apple   1.00          True        10     89.00            1.05\n",
      "1    Pear   1.50         False         0       NaN            1.58\n",
      "2   Melon   2.00         False         0    120.00            2.10\n",
      "3   Lemon   3.00          True        13    101.00            3.15\n"
     ]
    }
   ],
   "source": [
    "df[\"Price_with_Tax\"] = df[\"Price\"]*1.05\n",
    "print(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can apply the `.map()`method to change occurences of certain values to another one, which might be more descriptive in your opinion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     True\n",
      "1    False\n",
      "2    False\n",
      "3     True\n",
      "Name: Availability, dtype: bool\n",
      "0    Yes\n",
      "1     No\n",
      "2     No\n",
      "3    Yes\n",
      "Name: Availability, dtype: object\n",
      "0    Apple\n",
      "1     Pear\n",
      "2    Melon\n",
      "3    Lemon\n",
      "Name: Article, dtype: object\n",
      "0     Apfel\n",
      "1       NaN\n",
      "2    Melone\n",
      "3       NaN\n",
      "Name: Article, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df[\"Availability\"])\n",
    "mapping_dict = {False: \"No\", True: \"Yes\"}\n",
    "print(df[\"Availability\"].map(mapping_dict)) # Maps occurences of False to \"No\" and occurences of True to \"Yes\"\n",
    "\n",
    "print(df[\"Article\"])\n",
    "print(df[\"Article\"].map({\"Apple\": \"Apfel\", \"Melon\": \"Melone\"})) # Missing mappings become NaNs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Analysis\n",
    "\n",
    "Sums, Mean, Standard Deviation and Variance are also easily availabe for dataframes and series. For Dataframes the axis can be specified to calculate along a row or the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Article  Price  Availability  In Stock  Calories  Price_with_Tax\n",
      "0   Apple   1.00          True        10     89.00            1.05\n",
      "1    Pear   1.50         False         0       NaN            1.58\n",
      "2   Melon   2.00         False         0    120.00            2.10\n",
      "3   Lemon   3.00          True        13    101.00            3.15\n",
      "Sum:  7.5\n",
      "Mean:  1.875\n",
      "Std:  0.8539125638299665\n",
      "Var:  0.7291666666666666\n"
     ]
    }
   ],
   "source": [
    "print(df)\n",
    "print(\"Sum: \", df[\"Price\"].sum())\n",
    "print(\"Mean: \", df[\"Price\"].mean())\n",
    "print(\"Std: \", df[\"Price\"].std())\n",
    "print(\"Var: \", df[\"Price\"].var())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Filtering\n",
    "\n",
    "Dataframes support `boolean indexing`, which means that you choose which rows to select with a list of boolean values.\n",
    "Also lists with boolean values can be obtained by checking, if a series fulfills a condition.\n",
    "By combining these two properties we can effectively filter our data by our chosen conditions.\n",
    "\n",
    "Conditions can be combined with the and-operator `&`, the or-operator `|` and the not-operator `~` or simply the commands `and`, `or` and `not` themselves to form more complex conditions.\n",
    "\n",
    "Here we get the boolean array, where the price is grater than two, and then get the entries, which fulfill the condition. We only show relevant columns for each result.\n",
    "\n",
    "We also try another condition, where we only want to see articles, whose name contain an 'on'.\n",
    "\n",
    "The last example executes a complex filter in one go.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Article  Price  Calories  Availability\n",
      "0   Apple   1.00     89.00          True\n",
      "1    Pear   1.50       NaN         False\n",
      "2   Melon   2.00    120.00         False\n",
      "3   Lemon   3.00    101.00          True\n",
      "\n",
      "Greater two?\n",
      "0    False\n",
      "1    False\n",
      "2    False\n",
      "3     True\n",
      "Name: Price, dtype: bool\n",
      "  Article  Price\n",
      "3   Lemon   3.00\n",
      "\n",
      "Name with 'on'?\n",
      "2    Melon\n",
      "3    Lemon\n",
      "Name: Article, dtype: object\n",
      "\n",
      "Complex requirement fullfilled?\n",
      "  Article  Price  Calories  Availability\n",
      "0   Apple   1.00     89.00          True\n",
      "3   Lemon   3.00    101.00          True\n"
     ]
    }
   ],
   "source": [
    "# Original\n",
    "print(df[[\"Article\", \"Price\", \"Calories\", \"Availability\"]], end=\"\\n\\n\")\n",
    "\n",
    "# Filtered with simple boolean array\n",
    "print(\"Greater two?\")\n",
    "boolean_array = (df.Price > 2)\n",
    "print(boolean_array)\n",
    "filtered_df = df[boolean_array]\n",
    "print(filtered_df[[\"Article\", \"Price\"]], end=\"\\n\\n\")\n",
    "\n",
    "# String condition, article name must contain 'on'\n",
    "print(\"Name with 'on'?\")\n",
    "print(df[df.Article.str.contains(\"on\")].Article, end=\"\\n\\n\")\n",
    "\n",
    "# Filtered with complex condition\n",
    "# Only access cheap articles or with a lot of calories, which are not unavailable\n",
    "print(\"Complex requirement fullfilled?\")\n",
    "print(df[((df.Price <= 2.0) | (df.Calories >= 100)) & ~(df.Availability == False)][[\"Article\", \"Price\", \"Calories\", \"Availability\"]])\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Aggregation\n",
    "\n",
    "The last functionality we'll have a look at today is data aggregation. This is done via the `groupby()` function, which takes a column name or a list of column names as argument and returns an object, where all entries with the same value in the given column are summarized into one entry. This object by itself can't be easily printed, as it's a higher dimensional object, but we can combine it with an aggregation function, like `.first(), .last(), .mean(), .median(), .min(), .max(), sum(), .std(), .var()` or a custom function(for more see [here](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html)), to e.g. get the first value/mean/minimal/maximal value of each of the possible values of the column that you grouped by. The higher dimensional object collapses to a simple 2D representation again.\n",
    "\n",
    "This is the most advanced function, that we'll have a look at today, so let's have a look what happens here.\n",
    "\n",
    "We'll return to the `molecule_df`, which contained molecular and atomic properties, and aggregate it by the molecular index at first the access the molecular property `energies`.\n",
    "\n",
    "Also we can check how much the `charges` of each atom index vary in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of the aggregate: <class 'pandas.core.groupby.generic.DataFrameGroupBy'>\n",
      "Type of the aggreagete after aggregation_function application <class 'pandas.core.series.Series'>\n",
      "Length of the molecular energies: 13548\n",
      "Unique molecule idxs: 13548\n",
      "mol_idxs\n",
      "0   0.14\n",
      "1   0.13\n",
      "2   0.12\n",
      "3   0.12\n",
      "4   0.11\n",
      "Name: energies, dtype: float64\n",
      "at_idxs\n",
      "0    0.03\n",
      "1    5.50\n",
      "2    0.02\n",
      "3    0.03\n",
      "4    0.03\n",
      "5    0.03\n",
      "6    5.70\n",
      "7    0.03\n",
      "8    0.03\n",
      "9    0.03\n",
      "10   0.03\n",
      "11   5.50\n",
      "12   0.03\n",
      "13   0.02\n",
      "14   0.02\n",
      "Name: charges, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "molecule_aggregate = molecule_df.groupby(\"mol_idxs\")\n",
    "molecule_energies = molecule_aggregate[\"energies\"].first()\n",
    "print(\"Type of the aggregate:\", type(molecule_aggregate))\n",
    "print(\"Type of the aggreagete after aggregation_function application\", type(molecule_energies))\n",
    "print(\"Length of the molecular energies:\", len(molecule_energies))\n",
    "print(\"Unique molecule idxs:\", molecule_df[\"mol_idxs\"].nunique())\n",
    "print(molecule_energies.head())\n",
    "\n",
    "atom_aggregate = molecule_df.groupby(\"at_idxs\")\n",
    "charge_variance = atom_aggregate[\"charges\"].var()\n",
    "\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "print(charge_variance*100)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als letztes soll der Dataframe gespeichert werden. Dies kann u.a. als Text, als csv oder als Excel-Tabelle geschehen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Article  Price  Availability  In Stock  Calories  Price_with_Tax\n",
      "0   Apple   1.00          True        10     89.00            1.05\n",
      "1    Pear   1.50         False         0       NaN            1.58\n",
      "2   Melon   2.00         False         0    120.00            2.10\n",
      "3   Lemon   3.00          True        13    101.00            3.15\n"
     ]
    }
   ],
   "source": [
    "# Carful! Only execute, if you don't have any important data with the same name lying around\n",
    "path = \"test_tabelle.csv\" \n",
    "df.to_csv(path, sep=\";\", index=False) # sep delimites with semicolon, index=false drops the index-column\n",
    "df_reloaded = pd.read_csv(path, sep=\";\")\n",
    "print(df_reloaded)\n",
    "\n",
    "# Lösche die Tabelle wieder\n",
    "import os\n",
    "os.remove(path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**That's it**\n",
    "\n",
    "Thanks for participating in the Pandas crash course. There are countless further function, especially in combination with numpy, to execute complicated calculatiions and transformations in a few lines of code. Once you have your data in a readable format, is can be quickly analyzed descriptively. Together with matplotlib a well formatted and visually pleasing depiction is also easily createable automatically.\n",
    "\n",
    "Once the script is done, adjustments for small format changes for multiple data sets are quickly and consistently done without frustration to have to go to all formatting details again. Transfer to other data sets is as easy as changing the path variable.\n",
    "\n",
    "Don't forget that Pandas is a popular library and well documented, so if you run in any problems, they are likely to be well described somewhere."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your turn**\n",
    "\n",
    "If you want to try to apply your acquired knowledge: Try your hand at the pandas exercises in the second notebook."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "09479eb26e9c2dfedc5b750a5f4404419e7e88fc081728cd256fd3e13a96b5b1"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
